# -*- coding: utf-8 -*-
"""Copia de Copia de WebScraping_original modificado.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BAll3lQrCnrDD_YDT_3u-aFlDEvvmhtj

https://www.kaggle.com/code/scratchpad/notebook8622d098de/edit

hetulmehta, Hetul Mehta, Kaggle Expert, Mumbai, Maharashtra, India, Technical Head at DataZen
"""

# 0 ******************************************************************************************************************************
import requests
import pandas as pd
from joblib import load

# import warnings filter
from warnings import simplefilter
# ignore all future warnings
simplefilter(action='ignore', category=FutureWarning)
simplefilter(action='ignore', category=DeprecationWarning)

# 1 *******************************************************************************************************************************
class Model:
    
    def __init__(self): 
        #self.model = load('Back_end\Model\Sklearn\Model_A.pk')
        #self.vectorizer = load('Back_end\Vectorizer\Vectorizer_A.pk')
        #self.dictionary = load('Back_end\Dictionary\Id_to_category_dict_A.pk')
        self.model = load('Model/Sklearn/Model_A.pk')
        self.vectorizer = load('Vectorizer/Vectorizer_A.pk')
        self.dictionary = load('Dictionary/Id_to_category_dict_A.pk')

        

# 2 *******************************************************************************************************************************

    def visit_url(self, website_url):
        from bs4 import BeautifulSoup
        import requests
            
        #headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.71 Safari/537.36'}
        content = requests.get(website_url,timeout=60).content
        soup = BeautifulSoup(content, "lxml")
        result = {
            "website_url": website_url,
            "website_name": self.get_website_name(website_url),
            "website_text": self.get_html_title_tag(soup)+self.get_html_meta_tags(soup)+self.get_html_heading_tags(soup)+
                                                            self.get_text_content(soup)
        }
        return pd.Series(result)

    def get_website_name(self,website_url):
        from urllib.parse import urlparse
        return "".join(urlparse(website_url).netloc.split(".")[-2])

    def get_html_title_tag(self,soup):
        return '. '.join(soup.title.contents)

    def get_html_meta_tags(self,soup):
        tags = soup.find_all(lambda tag: (tag.name=="meta") & (tag.has_attr('name') & (tag.has_attr('content'))))
        content = [str(tag["content"]) for tag in tags if tag["name"] in ['keywords','description']]
        return ' '.join(content)

    def get_html_heading_tags(self,soup):
        tags = soup.find_all(["h1","h2","h3","h4","h5","h6"])
        content = [" ".join(tag.stripped_strings) for tag in tags]
        return ' '.join(content)

    def get_text_content(self,soup):
        import bs4 as bs4
        tags_to_ignore = ['style', 'script', 'head', 'title', 'meta', '[document]',"h1","h2","h3","h4","h5","h6","noscript"]
        tags = soup.find_all(text=True)
        result = []
        for tag in tags:
            stripped_tag = tag.strip()
            if tag.parent.name not in tags_to_ignore\
                and isinstance(tag, bs4.element.Comment)==False\
                and not stripped_tag.isnumeric()\
                and len(stripped_tag)>0:
                result.append(stripped_tag)
        return ' '.join(result)
    
    def jsonlink_api(self, url):
        api_key = 'pk_9e55e2c86db183a04eb6403d476e8350bb8a2580'

        params = {'url': url, 'api_key': api_key}
        response = requests.get('https://jsonlink.io/api/extract', params=params)

        if response.status_code == 200:
            data = response.json()
            #print(data)
            return data['title'], data['description'], data['domain'], data['favicon'], data['images'][0]
        else:
            print(f'Error: {response.status_code} - {response.text}')

# 3 **************************************************************************************************************************
            
    def clean_text(self, doc):
        import en_core_web_sm
        nlp = en_core_web_sm.load()
        doc = nlp(doc)
        tokens = []
        exclusion_list = ["nan"]
        for token in doc:
            if token.is_stop or token.is_punct or token.text.isnumeric() or (token.text.isalnum()==False) or token.text in exclusion_list :
                continue
            token = str(token.lemma_.lower().strip())
            tokens.append(token)
        return " ".join(tokens)

# 4 ***************************************************************************************************************************

    def predict(self, url):         
        try: 
            title, description, domain, icon, siteimage = self.jsonlink_api(url)
            print(title)
            web=dict(self.visit_url(url))
            text=(self.clean_text(web['website_text']))
            t=self.vectorizer.transform([text])
            print(self.dictionary)
            print(self.model.predict(t)[0])
            return title, description, domain, icon, siteimage, self.dictionary[self.model.predict(t)[0]]           
        except:
            print("No se puede establecer una conexi√≥n al website!")

# * ***************************************************************************************************************************




